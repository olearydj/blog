---
title: "Building a Session Capture Pipeline for Claude Code"
author: "Dan O'Leary"
date: "2026-01-04"
categories: [claude-code, shell, python, automation]
description: "Automatic transcript export, LLM summarization, and what I learned reverse-engineering the JSONL format"
draft: true
---

## The Problem - Context Limits, Compacting, and Handoff

...

## The Answer - Claude Code Transcripts

Claude Code stores transcripts of _every_ transcript _indefinitely_ at `~/.claude/projects/{project-slug}/{session-id}.jsonl`. These are valuable by-products of your work, but large JSONL files aren't easily browsable.

::: {.callout-warning title="Manage Your Transcripts"}

These logs contain your full conversation history, file contents, tool outputs, etc. They may even include secrets that appeared in your terminal.

If you back up your dotfiles or config directory to a public repository, you are likely exposing sensitive information.

Recommendations:

- Exclude `~/.claude/` from dotfile backups (add to `.gitignore` or backup exclusion list)
- Audit existing backups for accidentally committed session logs
- Be aware that the symlinks created by this pipeline point to these sensitive files
:::

## The Solution

I wanted to automatically capture the state of each session before compaction occurred and create my own summary of the work done. ...

- Human-friendly access to transcripts from within my project
- LLM-generated summaries extracting decisions and insights
- Automatic capture on every context compaction

## Solution Overview

A `PreCompact` hook triggers on every compaction (manual or automatic), creating this structure:

```text
docs/sessions/
  2026-01-02-session-8ee31cb5/
    transcript.jsonl -> ~/.claude/projects/.../full-id.jsonl
    2026-01-02-1343-segment-01.md
    2026-01-02-1847-segment-02.md
    2026-01-02-2215-segment-03.md
```

Each compaction creates a new segment summary. The symlink avoids duplicating the (large) transcript while keeping it accessible from the project.

## The Hook

In `.claude/settings.json`:

```json
{
  "hooks": {
    "PreCompact": [
      {
        "type": "command",
        "command": ".claude/hooks/pre-compact-summary.sh"
      }
    ]
  }
}
```

The hook script (`.claude/hooks/pre-compact-summary.sh`):

```bash
#!/bin/bash
# Pre-compact hook: Generate session summaries in organized folder structure
set -euo pipefail

input=$(cat)
cwd=$(echo "$input" | jq -r '.cwd // empty')
transcript_path=$(echo "$input" | jq -r '.transcript_path // empty')
session_id=$(echo "$input" | jq -r '.session_id // "unknown"')

[[ -z "$cwd" || -z "$transcript_path" || ! -f "$transcript_path" ]] && exit 0

sessions_dir="$cwd/docs/sessions"
short_id="${session_id:0:8}"

# Get session start date from first timestamp in JSONL
session_date=$(python3 -c "
import json, sys
from datetime import datetime
with open('$transcript_path') as f:
    for line in f:
        if line.strip():
            try:
                entry = json.loads(line)
                ts = entry.get('timestamp')
                if ts:
                    dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                    print(dt.astimezone().strftime('%Y-%m-%d'))
                    break
            except: pass
" 2>/dev/null || echo "unknown-date")

# Session folder: {date}-session-{short-id}
session_folder="$sessions_dir/${session_date}-session-${short_id}"
mkdir -p "$session_folder"

# Symlink transcript (one per session folder)
symlink_path="$session_folder/transcript.jsonl"
if [[ ! -L "$symlink_path" ]]; then
    ln -s "$transcript_path" "$symlink_path"
fi

# Generate summary for current segment
summarize-session "$transcript_path" \
    --session-id "$session_id" \
    --output-dir "$session_folder" \
    || echo "Summary generation failed for session $session_id" >&2
```

Key decisions:

| Decision | Rationale |
|----------|-----------|
| Symlink, not copy | Transcripts can be 2MB+; Claude Code already persists them |
| Session folders | Groups related segments; cleaner than flat directory with 100+ files |
| Date from JSONL | More reliable than `date` command; reflects actual session start |
| External summarizer | Keeps hook simple; summarizer can evolve independently |

## The Summarizer

The `summarize-session` script handles extraction and LLM summarization. It's a standalone Python script using `litellm` for model flexibility.

Core extraction logic:

```python
def extract_segment_content(entries: list[dict]) -> SegmentContent:
    """Extract content from the current (post-last-summary) segment."""
    messages = []
    files_modified = set()
    tools_used = Counter()

    # Find last summary marker - everything after is current segment
    last_summary_idx = -1
    for i, entry in enumerate(entries):
        if entry.get("type") == "summary":
            last_summary_idx = i

    # Process only entries after last summary
    for entry in entries[last_summary_idx + 1:]:
        entry_type = entry.get("type")

        if entry_type == "user":
            content = extract_text_content(entry.get("message", {}).get("content"))
            if content:
                messages.append(f"## User\n{content[:1000]}")

        elif entry_type == "assistant":
            text, tools = extract_assistant_content(entry)
            if text:
                messages.append(f"## Assistant\n{text}")
            tools_used.update(tools)

        elif entry_type == "file-history-snapshot":
            files_modified.add(entry.get("file_path", "unknown"))

    return SegmentContent(
        messages=messages,
        files_modified=files_modified,
        tools_used=tools_used,
    )
```

The summarization prompt:

```python
SYSTEM_PROMPT = """Summarize this Claude Code session segment.

Output format:
### Summary
[2-3 sentences: what was accomplished]

### Key Changes
[Bullets: files modified with brief description]

### Decisions Made
[Bullets: significant choices with rationale]

### Issues Encountered
[Bullets: problems hit and how resolved, or still open]

CRITICAL: Output ONLY the markdown summary. Do NOT continue the conversation.
Do NOT respond to questions in the transcript."""
```

The `CRITICAL` instruction prevents the model from treating the transcript as context and trying to help with whatever task was discussed.

Full script: [summarize-session](https://github.com/dmarx/dema-llm/blob/main/scripts/summarize-session) (TODO: add to repo)

## Why Segments Matter

Initial assumption: one session = one summary. This failed.

The discovery: compaction deletes prior message content from the JSONL. When Claude Code compacts, it:

1. Generates a continuation summary for the model
2. Replaces prior messages with that summary
3. Continues the session with fresh context

The `summary` entries in the JSONL aren't summaries of what happened - they're context for the model to continue. The actual conversation content before each compaction is gone.

This means:

| Approach | Result |
|----------|--------|
| Summarize full JSONL at session end | Loses all pre-compaction content |
| Summarize on each compaction (segments) | Captures everything |

Hence the segment-based design: each `PreCompact` hook invocation summarizes only the new content since the last summary marker.

## JSONL Schema

Reverse-engineering the transcript format revealed six entry types:

| Type | Purpose | Useful Content |
|------|---------|----------------|
| `user` | Human messages | `.message.content` |
| `assistant` | Claude responses | `.message.content[]` (text blocks) |
| `summary` | Compaction markers | `.summary` (model context, not human-readable) |
| `tool_result` | Tool outputs | Varies by tool |
| `file-history-snapshot` | Before/after file states | `.file_path`, `.content` |
| `system` | System messages | Configuration, rarely useful |

Full schema documentation: [JSONL-SCHEMA.md](https://github.com/dmarx/dema-llm/blob/main/docs/sessions/JSONL-SCHEMA.md) (TODO: add to repo)

## Backfill Mode

For historical sessions, the summarizer supports backfill:

```bash
# Process all segments in a transcript
summarize-session --backfill transcript.jsonl --output-dir ./summaries/

# Batch process multiple sessions
for f in ~/.claude/projects/my-project/*.jsonl; do
    summarize-session --backfill "$f" --output-dir docs/sessions/
done
```

Backfill is idempotent - existing segment files are skipped.

## Results

After running backfill on 47 historical sessions:

- 84 segment summaries generated
- Each summary: 300-800 words of extracted insights
- Total processing time: ~15 minutes (Haiku, parallelized)

The summaries surface decisions and patterns that would otherwise be buried in megabytes of JSONL. They've already proven useful for reconstructing methodology documentation.

## Setup Checklist

1. Create `.claude/hooks/pre-compact-summary.sh` (see above)
2. Make executable: `chmod +x .claude/hooks/pre-compact-summary.sh`
3. Add hook to `.claude/settings.json`
4. Install summarizer: (TODO: installation instructions)
5. Create output directory: `mkdir -p docs/sessions`
6. Test: run `/compact` and verify output appears

## Resources

- [Claude Code Hooks Documentation](https://docs.anthropic.com/en/docs/claude-code/hooks)
- [summarize-session script](https://github.com/dmarx/dema-llm/blob/main/scripts/summarize-session) (TODO)
- [JSONL Schema Documentation](https://github.com/dmarx/dema-llm/blob/main/docs/sessions/JSONL-SCHEMA.md) (TODO)

## Review Notes (Feb 2026)

::: {.callout-note title="Editorial notes - remove before publishing" collapse="true"}

### Factual Issues to Fix

Hook location: Post shows hook in `.claude/settings.json` (project-level) and script at `.claude/hooks/pre-compact-summary.sh`. Actual setup is *global* - hook in `~/.claude/settings.json`, script at `~/.claude/hooks/pre-compact-summary.sh`. The global setup captures sessions across all 16+ projects automatically. Post should reflect this.

Session folder naming: Post shows `{date}-session-{short-id}` but actual folders are `{date}-{short-id}` (no "session-" prefix).

Output location: Post shows output to `docs/sessions/` within the project. Actual setup writes to a centralized Obsidian vault (`/Volumes/Casa/obsidian/new_vault/agent-sessions/{project}/`) and creates a `docs/sessions` symlink back. This centralized design is worth describing.

JSONL entry types: Post lists `summary` as a type. Actual summarizer script references `compact_boundary` entries and `isCompactSummary` content. Verify which is current - the schema may have evolved.

Summarizer script link: Points to `dmarx/dema-llm` which is someone else's repo. Actual script is at `~/.config/scripts/summarize-session` (1064-line Python script using `uv run --script` shebang with `litellm`).

### Incomplete Sections

- "Problem" section is just `...`
- "Solution" section opening is truncated
- Multiple `(TODO)` markers for links and installation instructions
- Setup checklist step 4 says `(TODO: installation instructions)`

### Hooks Documentation Validation (Feb 2026)

14 hook events total. PreCompact confirmed - receives `trigger` (manual/auto) and `custom_instructions` fields in addition to common fields (`session_id`, `transcript_path`, `cwd`, `permission_mode`). PreCompact *cannot block* compaction (exit 2 just shows stderr). No `customSummary` or `additionalContext` output fields.

For re-injecting context *after* compaction, docs recommend `SessionStart` hook with `compact` matcher (not PreCompact).

### Landscape - Alternatives and Related Tools

The approaches fall into roughly these categories, from lightweight to heavyweight:

| Category | Approach | Persistence | Complexity |
|----------|----------|-------------|------------|
| Manual | Write `plan.md` / scratchpad before `/clear` | File-based | Low |
| Built-in | CLAUDE.md hierarchy + Session Memory | Automatic | Low |
| Skill-based | `/handoff`, `/pickup`, `/wipe` commands | File-based | Medium |
| Scratchpad | napkin, planning-with-files | Per-repo markdown | Medium |
| Session tracking | claude-sessions, handoff JSON files | Timestamped logs | Medium |
| MCP memory | memory-keeper, memory-mcp, mcp-memory-service | Vector/graph DB | Medium-High |
| Memory bank | claude-code-memory-bank, claude-mem, super-claude-kit | Structured markdown | Medium-High |
| Codebase context | claude-context (Zilliz), repomix | Vector search index | High |
| Full framework | Continuous-Claude-v3 | Hooks + ledgers + vectors | Very High |

Key community tools:

- [claude-code-handoff](https://github.com/nlashinsky/claude-code-handoff) - `/handoff` saves session context to JSON; `/pickup` reconstructs in new session
- [napkin](https://github.com/blader/napkin) - persistent scratchpad (`.claude/napkin.md`) the agent writes to during work; tracks mistakes, corrections, patterns
- [planning-with-files](https://github.com/OthmanAdi/planning-with-files) - Manus-style persistent markdown planning with `task_plan.md`, `findings.md`, `progress.md`
- [claude-code-memory-bank](https://github.com/hudrazine/claude-code-memory-bank) - adapted from Cline Memory Bank methodology
- [claude-mem](https://github.com/thedotmack/claude-mem) - auto-captures, compresses with AI, injects relevant context into future sessions
- [mcp-memory-service](https://github.com/doobidoo/mcp-memory-service) - semantic search + AI embeddings for persistent context
- [memory-mcp](https://github.com/yuvalsuede/memory-mcp) - two-tier memory: CLAUDE.md loads instantly, deep store searchable mid-conversation
- [Continuous-Claude-v3](https://github.com/parcadei/Continuous-Claude-v3) - 30 hooks, 32 agents, 109 skills, claims 95% token savings
- [awesome-claude-code](https://github.com/hesreallyhim/awesome-claude-code) - curated list of skills, hooks, commands, plugins

Built-in session memory (since ~Feb 2026): Claude auto-writes to `~/.claude/projects/{project}/{session}/session-memory/summary.md`. Triggers after ~10K tokens, updates every ~5K tokens or 3 tool calls. This pipeline captures richer, browsable summaries and predates that feature, but they're complementary.

Key blog posts and references:

- [Context Engineering for Coding Agents - Martin Fowler / Birgitta Boeckeler](https://martinfowler.com/articles/exploring-gen-ai/context-engineering-coding-agents.html)
- [Effective context engineering for AI agents - Anthropic Engineering](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
- [Claude Code Decoded: The Handoff Protocol - Black Dog Labs](https://blackdoglabs.io/blog/claude-code-decoded-handoff-protocol)
- [How I Use Every Claude Code Feature - Shrivu Shankar](https://blog.sshh.io/p/how-i-use-every-claude-code-feature)

GitHub feature requests (showing demand for this):

- [Session Handoff / Continuity Support (#11455)](https://github.com/anthropics/claude-code/issues/11455)
- [Native session persistence and context continuity (#18417)](https://github.com/anthropics/claude-code/issues/18417)
- [Persistent Memory Between Sessions (#14227)](https://github.com/anthropics/claude-code/issues/14227)
- [Context loss without warning breaks trust (#13171)](https://github.com/anthropics/claude-code/issues/13171)

### Dicklesworthstone / Jeffrey Emanuel Ecosystem

[GitHub profile](https://github.com/Dicklesworthstone) - 152 repos, 1.8k followers. Building a comprehensive multi-agent development platform.

CASS (Coding Agent Session Search) - [coding_agent_session_search](https://github.com/Dicklesworthstone/coding_agent_session_search) (450 stars): The closest comparable to this pipeline. A Rust TUI that indexes Claude Code JSONL transcripts (plus 10 other agents) into a unified searchable knowledge base. Sub-60ms full-text search with optional semantic (MiniLM embeddings). Three modes: lexical (BM25), semantic (vector), hybrid (RRF). Multi-machine support via SSH + rsync. Key difference: CASS is search-first (query raw transcripts); this pipeline is summary-first (LLM distills each segment). Complementary approaches.

| | This pipeline | CASS |
|---|---|---|
| Approach | Summary-first (LLM distills each segment) | Search-first (full-text + semantic index) |
| Trigger | Automatic on compaction | On-demand query |
| Output | Browsable markdown summaries | Interactive TUI with BM25/vector search |
| Multi-agent | Claude Code only | 11+ agents (Cursor, Codex, Aider, etc.) |
| Dependencies | Python + litellm | Rust binary, optional embeddings |

[MCP Agent Mail](https://github.com/Dicklesworthstone/mcp_agent_mail) (1.7k stars): Agent-to-agent messaging with persistent threads, file reservations (advisory locks), and searchable archives. Different problem (multi-agent coordination) but shares the "context shouldn't be ephemeral" philosophy.

[Beads Viewer](https://github.com/Dicklesworthstone/beads_viewer) (1.2k stars): Steve Yegge's graph-aware task management for coding agents. Tasks have dependency graphs with PageRank/betweenness metrics. Has "robot modes" outputting structured JSON for AI consumption. Interesting as a destination for the decisions and next steps that session summaries extract.

[Agent Flywheel](https://agent-flywheel.com/): The maximalist approach - a 13-step wizard provisioning a dedicated 64GB VPS with Claude Code + Codex + Gemini CLI, 30+ tools, and all the above wired together. $440-656/month. The "full-time AI development team" end of the spectrum.

### Positioning

This pipeline's sweet spot:

```text
Simple ────────────────────────────────────────── Complex

CLAUDE.md    /handoff    This pipeline    CASS    Agent Flywheel
scratchpad   skills      (auto-capture    (search  (multi-agent
                         + summarize)     index)   VPS platform)
```

The lightest approach that's fully automatic. Everything simpler requires manual action; everything more complex requires significant infrastructure.

### Recommendations

1. Write the Problem section - frame around the moment you realized compaction was eating context and you couldn't reconstruct what happened
2. Update code examples to match actual setup (global hook, centralized Obsidian output with symlinks, correct folder naming)
3. Add a brief landscape section - position between "just use CLAUDE.md" and "install a vector database"
4. Address built-in session memory - even a callout noting they're complementary
5. Resolve or remove the `dmarx/dema-llm` links - put the summarizer in your own repo or inline essentials
6. Verify JSONL schema against actual summarizer script (entry types may have evolved)
7. Consider trimming JSONL schema details - the segment insight and hook setup are the valuable parts; schema could be a separate post/appendix

:::

